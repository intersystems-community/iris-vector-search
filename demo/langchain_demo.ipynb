{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using InterSystems Vector Search with LangChain\n",
    "\n",
    "In this notebook, we'll leverage the Vector Search capabilities available in [InterSystems IRIS 2024.1](https://www.intersystems.com/news/iris-vector-search-support-ai-applications/) and [InterSystems IRIS Cloud SQL](https://developer.intersystems.com/products/iris-cloud-sql-integratedml/), using the well-known [LangChain](https://www.langchain.com/) framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the connection\n",
    "\n",
    "First, let's make sure we set up the connection to your InterSystems IRIS instance or Cloud SQL deployment. When targeting a Cloud SQL deployment, change the username and password to `SQLAdmin` and the corresponding password you chose when creating the deployment, and set the port to 443. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "username = 'demo'\n",
    "password = 'demo'\n",
    "hostname = os.getenv('IRIS_HOSTNAME', 'localhost')\n",
    "port = 1972 \n",
    "namespace = 'USER'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Securing the connection\n",
    "\n",
    "If the target you're connecting to requires secure connections, as is the case for Cloud SQL deployments, we need to supply a certificate and some additional settings to the driver. For Cloud SQL, you can download the certificate file from your deployment's details screen. Look for the button that says \"Get X.509 certificate\", and copy it into a local folder, such as `/usr/cert-demo/`. If you're running this notebook in a container, you can copy the certificate file into the container using the following command:\n",
    "\n",
    "```Shell\n",
    "docker cp ~/Downloads/certificateSQLaaS.pem iris-vector-search-jupyter-1:/usr/cert-demo/certificateSQLaaS.pem\n",
    "```\n",
    "\n",
    "Remember to also set the port to 443 in the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No certificate file found, continuing with insecure connection\n"
     ]
    }
   ],
   "source": [
    "import ssl\n",
    "\n",
    "certificateFile = \"/usr/cert-demo/certificateSQLaaS.pem\"\n",
    "\n",
    "if (os.path.exists(certificateFile)):\n",
    "    print(\"Located SSL certficate at '%s', initializing SSL configuration\", certificateFile)\n",
    "    sslcontext = ssl.create_default_context(cafile=certificateFile)\n",
    "else:\n",
    "    print(\"No certificate file found, continuing with insecure connection\")\n",
    "    sslcontext = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world!\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "url = f\"iris://{username}:{password}@{hostname}:{port}/{namespace}\"\n",
    "engine = create_engine(url, connect_args={\"sslcontext\": sslcontext})\n",
    "with engine.connect() as conn:\n",
    "    print(conn.execute(text(\"SELECT 'hello world!'\")).first()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating vectors using LangChain\n",
    "\n",
    "The following cell will load the `state_of_the_union.txt` file from the `/data/` directory and split it into chunks that are ready for translation into vectors, using standard LangChain components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "loader = TextLoader(\"../data/state_of_the_union.txt\", encoding='utf-8')\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=400, chunk_overlap=20)\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up your OpenAI API key\n",
    "\n",
    "If you have an OpenAI subscription, use the following cell to pick up your OpenAI API key, and use `OpenAIEmbeddings()` in the cells below. \n",
    "\n",
    "Alternatively, you can skip this step and use a local embeddings model that's included in the libraries already imported, such as `HuggingFaceEmbeddings()`, `FastEmbeddings()`, or `FakeEmbeddings()` (for testing purposes!). Just comment / uncomment the corresponding lines in the cells further down the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "OpenAI API Key: ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "if \"OPENAI_API_KEY\" in os.environ:\n",
    "    os.environ.pop(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"): \n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell, we'll put all the pieces together and create embeddings for our document collection and store them as a collection in our IRIS database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of docs in vector store: 114\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.embeddings import FakeEmbeddings\n",
    "from langchain.embeddings.fastembed import FastEmbedEmbeddings\n",
    "\n",
    "from langchain_iris import IRISVector\n",
    "\n",
    "db = IRISVector.from_documents(\n",
    "    embedding = OpenAIEmbeddings(), \n",
    "    # embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\"),\n",
    "    # embedding = FastEmbeddings(),\n",
    "    # embedding = FakeEmbeddings(size=123),\n",
    "    documents = docs,\n",
    "    collection_name = \"state_of_the_union_test\",\n",
    "    connection_string = f\"iris://{username}:{password}@{hostname}:{port}/{namespace}\",\n",
    "    engine_args = { \"connect_args\": {\"sslcontext\": sslcontext} }\n",
    ")\n",
    "\n",
    "print(f\"Number of docs in vector store: {len(db.get()['ids'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll use LangChain's similarity search API to retrieve documents from our collection that match a free text query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Score:  0.180285360830114\n",
      "And if we are to advance liberty and justice, we need to secure the Border and fix the immigration system. \n",
      "\n",
      "We can do both. At our border, we’ve installed new technology like cutting-edge scanners to better detect drug smuggling.  \n",
      "\n",
      "We’ve set up joint patrols with Mexico and Guatemala to catch more human traffickers.\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Score:  0.207157801882616\n",
      "So let’s not abandon our streets. Or choose between safety and equal justice. \n",
      "\n",
      "Let’s come together to protect our communities, restore trust, and hold law enforcement accountable. \n",
      "\n",
      "That’s why the Justice Department required body cameras, banned chokeholds, and restricted no-knock warrants for its officers.\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Score:  0.208728465122089\n",
      "There is so much we can do. Increase funding for prevention, treatment, harm reduction, and recovery.  \n",
      "\n",
      "Get rid of outdated rules that stop doctors from prescribing treatments. And stop the flow of illicit drugs by working with state and local law enforcement to go after traffickers.\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Score:  0.211304248421642\n",
      "We are choking off Russia’s access to technology that will sap its economic strength and weaken its military for years to come.  \n",
      "\n",
      "Tonight I say to the Russian oligarchs and corrupt leaders who have bilked billions of dollars off this violent regime no more. \n",
      "\n",
      "The U.S. Department of Justice is assembling a dedicated task force to go after the crimes of Russian oligarchs.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "query = \"Joint patrols to catch traffickers\"\n",
    "docs_with_score = db.similarity_search_with_score(query)\n",
    "\n",
    "for doc, score in docs_with_score:\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Score: \", score)\n",
    "    print(doc.page_content)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Document(metadata={}, page_content='dog'), 0.0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.add_documents([Document(page_content=\"dog\")])\n",
    "docs_with_score = db.similarity_search_with_score(\"dog\")\n",
    "docs_with_score[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
