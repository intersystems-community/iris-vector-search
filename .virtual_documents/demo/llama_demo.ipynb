





import os

username = 'demo'
password = 'demo'
hostname = os.getenv('IRIS_HOSTNAME', 'localhost')

port = 1972 
namespace = 'USER'





import ssl

certificateFile = "/usr/cert-demo/certificateSQLaaS.pem"

if (os.path.exists(certificateFile)):
    print("Located SSL certficate at '%s', initializing SSL configuration", certificateFile)
    sslcontext = ssl.create_default_context(cafile=certificateFile)
else:
    print("No certificate file found, continuing with insecure connection")
    sslcontext = None


from sqlalchemy import create_engine, text

url = f"iris://{username}:{password}@{hostname}:{port}/{namespace}"

engine = create_engine(url, connect_args={"sslcontext": sslcontext})
with engine.connect() as conn:
    print(conn.execute(text("SELECT 'hello world!'")).first()[0])





#from llama_index import SimpleDirectoryReader
from llama_index.legacy import SimpleDirectoryReader

documents = SimpleDirectoryReader("../data/paul_graham").load_data()
print("First Document ID:", documents[0].doc_id)





import getpass
import os
from dotenv import load_dotenv

load_dotenv(override=True)

if not os.environ.get("OPENAI_API_KEY"):
    os.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key:")





from llama_index.legacy import ServiceContext, set_global_service_context
from llama_index.embeddings.langchain import LangchainEmbedding
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.embeddings import FakeEmbeddings

if not os.environ.get("OPENAI_API_KEY"):
    lc_embed_model = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
    # lc_embed_model = FakeEmbeddings(size=1536)

    # ServiceContext captures how vectors will be generated
    service_context = ServiceContext.from_defaults(
        embed_model=LangchainEmbedding(lc_embed_model), 
        llm=None
    )

    set_global_service_context(service_context)





from llama_index.legacy import StorageContext
from llama_index.legacy.indices.vector_store import VectorStoreIndex
from llama_iris import IRISVectorStore

# StorageContext captures how vectors will be stored
vector_store = IRISVectorStore.from_params(
    connection_string = url,
    table_name = "paul_graham_essay",
    embed_dim = 1536,  # openai embedding dimension
    engine_args = { "connect_args": {"sslcontext": sslcontext} }
)
storage_context = StorageContext.from_defaults(vector_store=vector_store)





index = VectorStoreIndex.from_documents(
    documents, 
    storage_context=storage_context, 
    show_progress=True, 
)
query_engine = index.as_query_engine()


# # If reconnecting to the vector store, use this: 

index = VectorStoreIndex.from_vector_store(vector_store=vector_store)
storage_context = StorageContext.from_defaults(vector_store=vector_store)
query_engine = index.as_query_engine()

# Adding documents to existing index

for d in documents:
    index.insert(document=d, storage_context=storage_context)


response = query_engine.query("What did the author do?")


import textwrap
print(textwrap.fill(str(response), 100))


response = query_engine.query("What happened in the mid 1980s?")
print(textwrap.fill(str(response), 100))






